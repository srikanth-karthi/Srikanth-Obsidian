### ğŸ§®Â **What is Batch Size in Machine Learning?**

**Batch size**Â is aÂ **hyperparameter**Â that definesÂ **how many data samples**Â the model processesÂ **before updating**Â the weights and bias.

---

### ğŸš€Â **Why Not Use the Whole Dataset at Once?**

- In large datasets (e.g., millions of samples), calculating the loss and gradient for theÂ **entire dataset at once**Â (calledÂ **full-batch gradient descent**) isÂ **computationally expensive**Â andÂ **slow**.
    
- Instead, models useÂ **smaller batches**Â of data to approximate the gradients.
    

---

### âš–ï¸Â **Common Gradient Descent Strategies**

|Method|Batch Size|Characteristics|
|---|---|---|
|**Stochastic Gradient Descent (SGD)**|1|Fast, butÂ **noisy**; gradient updates after each example.|
|**Mini-Batch SGD**|Between 2 and total number of examples|**Balanced**Â approachâ€”less noise, faster convergence.|
|**Batch Gradient Descent**|Entire dataset|Smooth butÂ **slow and memory-heavy**.|

---

### ğŸ“‰Â **Loss Curve Behavior**

- **SGD:**Â Loss curve isÂ **jittery**Â (lots of ups and downs), especially during early and late training stages.
    
- **Mini-Batch SGD:**Â Loss curve isÂ **smoother**Â withÂ **minor fluctuations**, leading toÂ **more stable convergence**.
    

---

### ğŸ’¡Â **Key Insight**

- **Small batch size**Â = more noise = better generalization (can escape local minima).
    
- **Large batch size**Â = stable but may lead to poor generalization.


next [[Epoch]]